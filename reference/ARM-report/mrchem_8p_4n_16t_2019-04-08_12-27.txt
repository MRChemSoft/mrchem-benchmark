Command:        mpirun ../../install-hybrid/bin/mrchem.x @mrchem.inp
Resources:      4 nodes (32 physical, 64 logical cores per node)
Tasks:          8 processes, OMP_NUM_THREADS was 16
Machine:        c12-12.cluster
Start time:     man. apr. 8 2019 12:27:15 (UTC+02)
Total time:     693 seconds (about 12 minutes)
Full path:      /cluster/home/stig/benchmarks-mrchem/ref/install-hybrid/bin

Summary: mrchem.x is Compute-bound in this configuration
Compute:                                     62.1% |=====|
MPI:                                         37.7% |===|
I/O:                                          0.2% ||
This application run was Compute-bound. A breakdown of this time and advice for investigating further is in the CPU section below. 

CPU:
A breakdown of the 62.1% CPU time:
Single-core code:                            21.1% |=|
OpenMP regions:                              78.9% |=======|
Scalar numeric ops:                           9.4% ||
Vector numeric ops:                          12.4% ||
Memory accesses:                             78.0% |=======|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
Little time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 37.7% MPI time:
Time in collective calls:                    37.3% |===|
Time in point-to-point calls:                62.7% |=====|
Effective process collective rate:            25.3 kB/s
Effective process point-to-point rate:        1.40 GB/s
Most of the time is spent in point-to-point calls with a high transfer rate. It may be possible to improve this further by overlapping communication and computation or reducing the amount of communication required.
The collective transfer rate is very low. This suggests load imbalance is causing synchronization overhead; use an MPI profiler to investigate.

I/O:
A breakdown of the 0.2% I/O time:
Time in reads:                                0.0% |
Time in writes:                             100.0% |=========|
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                 2.72 MB/s
Most of the time is spent in write operations with a very low effective transfer rate. This may be caused by contention for the filesystem or inefficient access patterns. Use an I/O profiler to investigate which write calls are affected.

OpenMP:
A breakdown of the 78.9% time in OpenMP regions:
Computation:                                 83.6% |=======|
Synchronization:                             16.4% |=|
Physical core utilization:                  100.0% |=========|
System load:                                 58.8% |=====|
OpenMP thread performance looks good. Check the CPU breakdown for advice on improving code efficiency.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                    3.54 GiB
Peak process memory usage:                    8.59 GiB
Peak node memory usage:                      25.0% |==|
There is significant variation between peak and mean memory usage. This may be a sign of workload imbalance or a memory leak.
The peak node memory usage is very low. Running with fewer MPI processes and more data on each process may be more efficient.

Energy:
A breakdown of how the 121 Wh was used:
CPU:                                        100.0% |=========|
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
The whole system energy has been calculated using the CPU energy usage.
System power metrics: No Arm IPMI Energy Agent config file found in /var/spool/ipmi-energy-agent. Did you start the Arm IPMI Energy Agent?

